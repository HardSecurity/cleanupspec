/*
 * Copyright (c) 1999-2013 Mark D. Hill and David A. Wood
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

machine(MachineType:L1Cache, "MESI Directory L1 Cache CMP")
: Sequencer * sequencer;
CacheMemory * L1Icache;
CacheMemory * L1Dcache;
CacheMemory * L2cache;

Prefetcher * prefetcher;
int l2_select_num_bits;
Cycles l1_request_latency := 2;
Cycles l1_response_latency := 2;
Cycles to_l2_latency := 1;
bool send_evictions;
bool enable_prefetch := "False";

// Message Queues
// From this node's L1 cache TO the network

// a local L1 -> this L2 bank, currently ordered with directory forwarded requests
MessageBuffer * requestFromL1Cache, network="To", virtual_network="0",
  vnet_type="request";

// a local L1 -> this L2 bank
MessageBuffer * responseFromL1Cache, network="To", virtual_network="1",
  vnet_type="response";

MessageBuffer * unblockFromL1Cache, network="To", virtual_network="2",
  vnet_type="unblock";


// To this node's L1 cache FROM the network
// a L2 bank -> this L1
MessageBuffer * requestToL1Cache, network="From", virtual_network="2",
  vnet_type="request";

// a L2 bank -> this L1
MessageBuffer * responseToL1Cache, network="From", virtual_network="1",
  vnet_type="response";

// Request Buffer for prefetches
MessageBuffer * optionalQueue;

// Buffer for requests generated by the processor core.
MessageBuffer * mandatoryQueue;
{
  // STATES
  state_declaration(State, desc="Cache states", default="L1Cache_State_I") {
    // Base states
    NP, AccessPermission:Invalid, desc="Not present in either cache";
    I, AccessPermission:Invalid, desc="a L1 cache entry Idle";
    S, AccessPermission:Read_Only, desc="a L1 cache entry Shared";
    E, AccessPermission:Read_Only, desc="a L1 cache entry Exclusive";
    M, AccessPermission:Read_Write, desc="a L1 cache entry Modified", format="!b";
    X, AccessPermission:Read_Only, desc="a L1 cache entry Speculatively observed";

    // Transient States
    IS, AccessPermission:Busy, desc="L1 idle, issued GETS, have not seen response yet";
    IM, AccessPermission:Busy, desc="L1 idle, issued GETX, have not seen response yet";
    IX, AccessPermission:Busy, desc="L1 idle, issued GETSPEC, have not seen response yet";
    SM, AccessPermission:Read_Only, desc="L1 idle, issued GETX, have not seen response yet";
    IS_I, AccessPermission:Busy, desc="L1 idle, issued GETS, saw Inv before data because directory doesn't block on GETS hit";

    M_I, AccessPermission:Busy, desc="L1 replacing, waiting for ACK";
    SINK_WB_ACK, AccessPermission:Busy, desc="This is to sink WB_Acks from L2";

    // Transient States in which block is being prefetched
    PF_IS, AccessPermission:Busy, desc="Issued GETS, have not seen response yet";
    PF_IM, AccessPermission:Busy, desc="Issued GETX, have not seen response yet";
    PF_SM, AccessPermission:Busy, desc="Issued GETX, received data, waiting for acks";
    PF_IS_I, AccessPermission:Busy, desc="Issued GETs, saw inv before data";

  }

  // EVENTS
  enumeration(Event, desc="Cache events") {
    // L1 events
    Delay_Load_RaceCondition,   desc="Load Issued when Load also in Progress";
    Load,            desc="Load request from the home processor";
    Ifetch,          desc="I-fetch request from the home processor";
    Store,           desc="Store request from the home processor";
    SpecLoad,        desc="SpecLoad request from the home processor";
    Expose,          desc="Expose request from the home processor";

    Inv,           desc="Invalidate request from L2 bank";
    
    // internal generated request
    L1_Replacement,  desc="L1 Replacement", format="!r";
    L1_Replacement_OnInstall,  desc="L1 Replacement Only When Install Returns", format="!r";
    L1_Replacement_VictimBusy, desc="L1 Replacement Failed (Recycle). Victim picked is budy.", format="!r"; 

    // other requests
    Fwd_GETX,   desc="GETX from other processor";
    Fwd_GETS,   desc="GETS from other processor";
    Fwd_GET_INSTR,   desc="GET_INSTR from other processor";
    Fwd_GETSPEC,   desc="GETSPEC from other processor";
    Fwd_EXPOSE,   desc="EXPOSE from other processor";

    Data,       desc="Data for processor";
    Data_Exclusive,       desc="Data for processor";
    DataS_fromL1,       desc="data for GETS request, need to unblock directory";
    Data_all_Acks,       desc="Data for processor, all acks";
    AllocateLine_OnInstall, desc="Allocate Line, when Install done & Data received";

    Ack,        desc="Ack for processor";
    Ack_all,      desc="Last ack for processor";

    WB_Ack,        desc="Ack for replacement";

    PF_Load,    desc="load request from prefetcher";
    PF_Ifetch,  desc="instruction fetch request from prefetcher";
    PF_Store,   desc="exclusive load request from prefetcher";

    // [CleanupCache]
    Waiting_CLEANUP, desc="Waiting on a Cleanup request";
    Inv_CLEANUP,   desc="InvalidateL1 request on a CleanupReq from processor";
    Done_Invalidate_CLEANUP, desc="Done InvalidateL1 request on a cleanupreq from processor";    
    Skip_ROLLBACK,  desc="Skip Rollback of Line in L1-DCache, request from processor";    
    Load_ROLLBACK, desc="Issue Rollback of Line in L1-Cache, request from processor";    
    Waiting_Rollback_CLEANUP, desc="Waiting for Rollback to be fired for L1-cache, on a cleanup from processor";
    InvL2_CLEANUP, desc="InvalidateL2 request on a CleanupReq from processor";
    L2InvAckRecvd, desc="Ack to a InvalidateL2 request  from cache";
    Done_CLEANUP, desc="Finished Cleanup on a CleanupReq from processor";
    Waiting_Final_CLEANUP, desc="Fired all the required Cleanup Actions & Waiting to Finish, on a CleanupReq from processor";
    Waiting_CLEANUP_inprogress,  desc="Data request Waiting for Cleanup Inprogress to complete";

    // [CleanupCache]: Waiting for Replacement to Finish for a Conflicting Line.
    Waiting_REPL_inprogress,  desc="Data install Waiting for Repl Inprogress to complete";

    // [CleanupCache]: Inflight Cleanup Events, without waiting for Line Installs.
    DropIncomingPacket, desc="Incoming Responses need to be dropped, because Line Has Dissappeared.";
    DropIncomingPacketnUnblockL2, desc="Incoming Responses need to be dropped, because Line Has Dissappeared. Also need to send unblock req to L2";
     DropIncomingPacketnUnblockRevertL2, desc="Incoming Responses need to be dropped, because Line Has Dissappeared. Also need to send unblock req to L2 and ask it to revert state";
     
    Inflight_CLEANUP_reqs, desc="Requests will be Sent to Caches for inflight cleanup.";
    Inflight_CLEANUP_resp, desc="Reponse received from L2 for inflight cleanup. Cleanup will complete.";
    Drop_Load_Cleanup_Inflight, desc="On a Inflight Cleanup, Load that is not issued should be dropped";
    Skip_Inflight_Cleanup, desc="When a Load hasnt been Issued, Cleanup Should be Skipped & Ack issued immediately";
  }

  // TYPES

  // CacheEntry
  structure(Entry, desc="...", interface="AbstractCacheEntry" ) {
    State CacheState,        desc="cache state";
    DataBlock DataBlk,       desc="data for the block";
    bool Dirty, default="false",   desc="data is dirty";
    bool isPrefetch, desc="Set if this block was prefetched";
  }

  // TBE fields
  structure(TBE, desc="...") {
    Addr addr,              desc="Physical address for this TBE";
    State TBEState,        desc="Transient state";
    DataBlock DataBlk,                desc="Buffer for the data block";
    bool Dirty, default="false",   desc="data is dirty";
    bool isPrefetch,       desc="Set if this was caused by a prefetch";
    int pendingAcks, default="0", desc="number of pending acks";
    int max_depth,default="-1", desc="Level to which packet went to fetch the data";
    bool Rollback, default ="false", desc="Is TBE for a in-progress Rollback?";
    bool L1Invalidated, default ="false", desc="Is TBE for a in-progress L1Invalidate?";
    int ct_id, default="-1", desc="Cleanup Table ID, if TBE is associated with a cleanup";
    bool InvL2_AckReq, default= "false", desc="is TBE for a in-progress L2 invalidate"    ;

    bool lineInL2, default="false", desc="is L2 Invalidation required, when cleanup inflight issued";
    bool Inflight_cleanup_recvd, default="false", desc="is Inflight Cleanup Request received, if yes, no install should happen ";
    bool cleanup_inflight_done, default="false", desc="Is cleanupAck issued & cleanup inflight done?";

    //flags for cleanup_inflight, where Line Hits in L2.
    bool need_wait_L2ret_cleanup_inflight, default="false", desc="When lineInL2==true, Need to wait for the L2 Hit to Return Before CleanupAck For Cleanup Inflight.";
    bool received_L2ret_cleanup_inflight, default="false", desc="Received L2 Hit, Can proceed iwth cleanupAck for Cleanup Inflight";

  }

  structure(TBETable, external="yes") {
    TBE lookup(Addr);
    void allocate(Addr);
    void deallocate(Addr);
    bool isPresent(Addr);
  }

  TBETable TBEs, template="<L1Cache_TBE>", constructor="m_number_of_TBEs";

  int l2_select_low_bit, default="RubySystem::getBlockSizeBits()";

  Tick clockEdge();
  Cycles ticksToCycles(Tick t);
  Tick cyclesToTicks(Cycles c);
  void set_cache_entry(AbstractCacheEntry a);
  void unset_cache_entry();
  void set_tbe(TBE a);
  void unset_tbe();
  void wakeUpBuffers(Addr a);
  void profileMsgDelay(int virtualNetworkType, Cycles c);

  // inclusive cache returns L1 entries only
  Entry getCacheEntry(Addr addr), return_by_pointer="yes" {
    Entry L1Dcache_entry := static_cast(Entry, "pointer", L1Dcache[addr]);
    if(is_valid(L1Dcache_entry)) {
      return L1Dcache_entry;
    }

    Entry L1Icache_entry := static_cast(Entry, "pointer", L1Icache[addr]);
    return L1Icache_entry;
  }

  Entry getL1DCacheEntry(Addr addr), return_by_pointer="yes" {
    Entry L1Dcache_entry := static_cast(Entry, "pointer", L1Dcache[addr]);
    return L1Dcache_entry;
  }

  Entry getL1ICacheEntry(Addr addr), return_by_pointer="yes" {
    Entry L1Icache_entry := static_cast(Entry, "pointer", L1Icache[addr]);
    return L1Icache_entry;
  }

  State getState(TBE tbe, Entry cache_entry, Addr addr) {
    // [InvisiSpec] The same cache line cannot be present in L1D and L1I at the same time.
    assert((L1Dcache.isTagPresent(addr) && L1Icache.isTagPresent(addr)) == false);

    if(is_valid(tbe)) {
      return tbe.TBEState;
    } else if (is_valid(cache_entry)) {
      return cache_entry.CacheState;
    }
    return State:NP;
  }

  void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {
    assert((L1Dcache.isTagPresent(addr) && L1Icache.isTagPresent(addr)) == false);

    // MUST CHANGE
    if(is_valid(tbe)) {
    tbe.TBEState := state;
    }

    if (is_valid(cache_entry)) {
    cache_entry.CacheState := state;
    }
  }

  //[CleanupCache]: Get/Set Level where Packet was found (L2/Mem)
  int get_max_depth(TBE tbe){
    return tbe.max_depth;
  }

  void set_max_depth(TBE tbe, int max_depth){
  tbe.max_depth := max_depth;
  }

  AccessPermission getAccessPermission(Addr addr) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      DPRINTF(RubySlicc, "%s\n", L1Cache_State_to_permission(tbe.TBEState));
      return L1Cache_State_to_permission(tbe.TBEState);
    }

    Entry cache_entry := getCacheEntry(addr);
    if(is_valid(cache_entry)) {
      DPRINTF(RubySlicc, "%s\n", L1Cache_State_to_permission(cache_entry.CacheState));
      return L1Cache_State_to_permission(cache_entry.CacheState);
    }

    DPRINTF(RubySlicc, "%s\n", AccessPermission:NotPresent);
    return AccessPermission:NotPresent;
  }

  void functionalRead(Addr addr, Packet *pkt) {
    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
      testAndRead(addr, tbe.DataBlk, pkt);
    } else {
      testAndRead(addr, getCacheEntry(addr).DataBlk, pkt);
    }
  }

  int functionalWrite(Addr addr, Packet *pkt) {
    int num_functional_writes := 0;

    TBE tbe := TBEs[addr];
    if(is_valid(tbe)) {
    num_functional_writes := num_functional_writes +
        testAndWrite(addr, tbe.DataBlk, pkt);
      return num_functional_writes;
    }

  num_functional_writes := num_functional_writes +
      testAndWrite(addr, getCacheEntry(addr).DataBlk, pkt);
    return num_functional_writes;
  }

  void setAccessPermission(Entry cache_entry, Addr addr, State state) {
    if (is_valid(cache_entry)) {
      cache_entry.changePermission(L1Cache_State_to_permission(state));
    }
  }

  Event mandatory_request_type_to_event(RubyRequestType type) {
    if (type == RubyRequestType:LD) {
      return Event:Load;
    } else if (type == RubyRequestType:IFETCH) {
      return Event:Ifetch;
    } else if ((type == RubyRequestType:ST) || (type == RubyRequestType:ATOMIC)) {
      return Event:Store;
    } else if (type == RubyRequestType:SPEC_LD) {
      return Event:SpecLoad;
    } else if (type == RubyRequestType:EXPOSE) {
      return Event:Expose;
    } else {
      error("Invalid RubyRequestType");
    }
  }

  Event prefetch_request_type_to_event(RubyRequestType type) {
    if (type == RubyRequestType:LD) {
      return Event:PF_Load;
    } else if (type == RubyRequestType:IFETCH) {
      return Event:PF_Ifetch;
    } else if ((type == RubyRequestType:ST) ||
               (type == RubyRequestType:ATOMIC)) {
      return Event:PF_Store;
    } else {
      error("Invalid RubyRequestType");
    }
  }

  int getPendingAcks(TBE tbe) {
    return tbe.pendingAcks;
  }

  out_port(requestL1Network_out, RequestMsg, requestFromL1Cache);
  out_port(responseL1Network_out, ResponseMsg, responseFromL1Cache);
  out_port(unblockNetwork_out, ResponseMsg, unblockFromL1Cache);
  out_port(optionalQueue_out, RubyRequest, optionalQueue);


  // Prefetch queue between the controller and the prefetcher
  // As per Spracklen et al. (HPCA 2005), the prefetch queue should be
  // implemented as a LIFO structure.  The structure would allow for fast
  // searches of all entries in the queue, not just the head msg. All
  // msgs in the structure can be invalidated if a demand miss matches.
  in_port(optionalQueue_in, RubyRequest, optionalQueue, desc="...", rank = 3) {
    if (optionalQueue_in.isReady(clockEdge())) {
      peek(optionalQueue_in, RubyRequest) {
        // Instruction Prefetch
        if (in_msg.Type == RubyRequestType:IFETCH) {
          Entry L1Icache_entry := getL1ICacheEntry(in_msg.LineAddress);
          if (is_valid(L1Icache_entry)) {
            // The block to be prefetched is already present in the
            // cache. We should drop this request.
            trigger(prefetch_request_type_to_event(in_msg.Type),
                    in_msg.LineAddress,
                    L1Icache_entry, TBEs[in_msg.LineAddress]);
          }

          // Check to see if it is in the OTHER L1
          Entry L1Dcache_entry := getL1DCacheEntry(in_msg.LineAddress);
          if (is_valid(L1Dcache_entry)) {
            // The block is in the wrong L1 cache. We should drop
            // this request.
            trigger(prefetch_request_type_to_event(in_msg.Type),
                    in_msg.LineAddress,
                    L1Dcache_entry, TBEs[in_msg.LineAddress]);
          }

          if (L1Icache.cacheAvail(in_msg.LineAddress)) {
            // L1 does't have the line, but we have space for it
            // in the L1 so let's see if the L2 has it
            trigger(prefetch_request_type_to_event(in_msg.Type),
                    in_msg.LineAddress,
                    L1Icache_entry, TBEs[in_msg.LineAddress]);
          } else {
            // No room in the L1, so we need to make room in the L1
            Addr victim_addr := L1Icache.cacheProbe(in_msg.LineAddress,false);
            trigger(Event:L1_Replacement,
                    victim_addr,
                    getL1ICacheEntry(victim_addr),
                    TBEs[victim_addr]);
          }
        } else {
          // Data prefetch
          Entry L1Dcache_entry := getL1DCacheEntry(in_msg.LineAddress);
          if (is_valid(L1Dcache_entry)) {
            // The block to be prefetched is already present in the
            // cache. We should drop this request.
            trigger(prefetch_request_type_to_event(in_msg.Type),
                    in_msg.LineAddress,
                    L1Dcache_entry, TBEs[in_msg.LineAddress]);
          }

          // Check to see if it is in the OTHER L1
          Entry L1Icache_entry := getL1ICacheEntry(in_msg.LineAddress);
          if (is_valid(L1Icache_entry)) {
            // The block is in the wrong L1. Just drop the prefetch
            // request.
            trigger(prefetch_request_type_to_event(in_msg.Type),
                    in_msg.LineAddress,
                    L1Icache_entry, TBEs[in_msg.LineAddress]);
          }

          if (L1Dcache.cacheAvail(in_msg.LineAddress)) {
            // L1 does't have the line, but we have space for it in
            // the L1 let's see if the L2 has it
            trigger(prefetch_request_type_to_event(in_msg.Type),
                    in_msg.LineAddress,
                    L1Dcache_entry, TBEs[in_msg.LineAddress]);
          } else {
            // No room in the L1, so we need to make room in the L1
            Addr victim_addr :=  L1Dcache.cacheProbe(in_msg.LineAddress,false);
            if(L1Dcache.isAddrValid(victim_addr)){
              trigger(Event:L1_Replacement,
                      victim_addr,
                      getL1DCacheEntry(victim_addr),
                      TBEs[victim_addr]);
            } else{
              DPRINTF(RubySlicc, "**For Data Addr:%#x, REPL Stalled till parallel REPL inprogress completes\n",in_msg.LineAddress);
              trigger(Event:Waiting_REPL_inprogress, in_msg.LineAddress,
                      L1Dcache_entry, TBEs[in_msg.LineAddress]);
            }
          }
        }
      }
    }
  }

  // Response  L1 Network - response msg to this L1 cache
  in_port(responseL1Network_in, ResponseMsg, responseToL1Cache, rank = 2) {
    if (responseL1Network_in.isReady(clockEdge())) {
      peek(responseL1Network_in, ResponseMsg, block_on="addr") {
        assert(in_msg.Destination.isElement(machineID));
        
        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        if(!TBEs.isPresent(in_msg.addr)){
          DPRINTF(RubySlicc,"Response Queue has Addr:%#x (State:%s) - MsgType:%s from Src:%s, TBEIsPresent:%d\n",in_msg.addr,getState(tbe, cache_entry, in_msg.addr),in_msg.Type,in_msg.Sender,TBEs.isPresent(in_msg.addr));
        } else {
          DPRINTF(RubySlicc,"Response Queue has Addr:%#x (State:%s) - MsgType:%s,  from Src:%s, TBEIsPresent:%d. Cleanup_Seen:%d \n",in_msg.addr,getState(tbe, cache_entry, in_msg.addr),in_msg.Type,in_msg.Sender,TBEs.isPresent(in_msg.addr),tbe.Inflight_cleanup_recvd || L1Dcache.cleanup_inflight_received(in_msg.addr) );
        }

        if(is_invalid(tbe)){
          if( (in_msg.Type ==  CoherenceResponseType:DATA_EXCLUSIVE)
              ||  (in_msg.Type ==  CoherenceResponseType:DATA)
              || (in_msg.Type ==  CoherenceResponseType:ACK) ){

            DPRINTF(RubySlicc, "** Triggerring DropIncomingPacketNUnblokcRevertL2 for Addr: %#x in State: %s, TBEPresent:%d, msgType:%s, Sender:%s\n",in_msg.addr,getState(tbe, cache_entry, in_msg.addr),TBEs.isPresent(in_msg.addr),in_msg.Type,in_msg.Sender);
            trigger(Event:DropIncomingPacketnUnblockRevertL2, in_msg.addr,cache_entry, tbe);
          }
          else  {
            trigger(Event:DropIncomingPacket, in_msg.addr,cache_entry, tbe);
          }
          
        } else if ((tbe.Inflight_cleanup_recvd || L1Dcache.cleanup_inflight_received(in_msg.addr)) && (in_msg.Type !=  CoherenceResponseType:INV_ACK) ){
          if( (in_msg.Type ==  CoherenceResponseType:DATA_EXCLUSIVE)
              ||  (in_msg.Type ==  CoherenceResponseType:DATA)
              || (in_msg.Type ==  CoherenceResponseType:ACK) ) {

            DPRINTF(RubySlicc, "** Triggerring DropIncomingPacketNUnblokcRevertL2 for Addr: %#x in State: %s, TBEPreset:%d\n",in_msg.addr,getState(tbe, cache_entry, in_msg.addr),TBEs.isPresent(in_msg.addr));
            trigger(Event:DropIncomingPacketnUnblockRevertL2, in_msg.addr,cache_entry, tbe);

          }
          else { 
            
            if( !((in_msg.Type == CoherenceResponseType:WB_ACK) &&  (getState(tbe,cache_entry,in_msg.addr) == State:M_I )) ) { 
              trigger(Event:DropIncomingPacket, in_msg.addr,cache_entry, tbe);
            }   
          }
        }
        
        if(in_msg.Type == CoherenceResponseType:INV_ACK){
          DPRINTF(RubySlicc, "L2 Inv_Ack Response triggered for Addr: %#x in State: %s\n",
                  in_msg.addr,getState(tbe, cache_entry, in_msg.addr));

          if(tbe.Inflight_cleanup_recvd){
            //Cleanup Inflight Load
            trigger(Event:Inflight_CLEANUP_resp, in_msg.addr, cache_entry, tbe);
          } else {
            //Cleanup Executed Load
            trigger(Event:L2InvAckRecvd, in_msg.addr, cache_entry, tbe);
          }
          
        } else if(in_msg.Type == CoherenceResponseType:DATA_EXCLUSIVE) {
          //If Entry Valid, Trigger Install.
          if(is_valid(cache_entry)){
            set_max_depth(tbe,in_msg.max_depth);
            DPRINTF(RubySlicc, "Data_Exclusive:for Addr: %#x in State: %s. Install As Cache Entry Valid\n",
                  in_msg.addr,getState(tbe, cache_entry, in_msg.addr));

            trigger(Event:Data_Exclusive, in_msg.addr, cache_entry, tbe);
          } else {
            //Cache Has Space Avail, Then Allot Cache Entry & Reexecute.
            if (L1Dcache.cacheAvail(in_msg.addr)) {
              assert(getState(tbe, cache_entry, in_msg.addr) == State:NP);
              DPRINTF(RubySlicc, "Data_Exclusive:for Addr: %#x in State: %s. Install As Cache Space Avail\n",
                  in_msg.addr,getState(tbe, cache_entry, in_msg.addr));

              trigger(Event:AllocateLine_OnInstall,in_msg.addr,
                      cache_entry,tbe);
            }
            //Else Trigger Replacement to Make Space
            else {                        
              assert(getState(tbe, cache_entry, in_msg.addr) == State:NP);
              Addr victim_addr :=  L1Dcache.cacheProbe(in_msg.addr,true);

              if(L1Dcache.isAddrValid(victim_addr)){
                DPRINTF(RubySlicc, "Data_Exclusive:for Addr: %#x in State: %s. Victim :%#x being Replaced\n",
                        in_msg.addr,getState(tbe, cache_entry, in_msg.addr),victim_addr);
                trigger(Event:L1_Replacement_OnInstall,
                        victim_addr,
                        getL1DCacheEntry(victim_addr),
                        TBEs[victim_addr]);
              } else {
                DPRINTF(RubySlicc, "Data_Exclusive:for Addr: %#x in State: %s. Victim :%#x Not VALID! Recycle\n", 
                        in_msg.addr,getState(tbe, cache_entry, in_msg.addr),victim_addr);
                trigger(Event:L1_Replacement_VictimBusy, in_msg.addr, cache_entry,tbe);             

              }
            }
          }
        } else if(in_msg.Type == CoherenceResponseType:DATA) {
          if ((getState(tbe, cache_entry, in_msg.addr) == State:IS ||
               getState(tbe, cache_entry, in_msg.addr) == State:NP ||
               getState(tbe, cache_entry, in_msg.addr) == State:IX ||
               getState(tbe, cache_entry, in_msg.addr) == State:IS_I ||
               getState(tbe, cache_entry, in_msg.addr) == State:PF_IS ||
               getState(tbe, cache_entry, in_msg.addr) == State:PF_IS_I) &&
              machineIDToMachineType(in_msg.Sender) == MachineType:L1Cache) {

            if(is_valid(cache_entry)){
              set_max_depth(tbe,in_msg.max_depth);
              trigger(Event:DataS_fromL1, in_msg.addr, cache_entry, tbe);
            } else {
              //Cache Has Space Avail, Then Allot Cache Entry & Reexecute.
              if (L1Dcache.cacheAvail(in_msg.addr)) {
                assert(getState(tbe, cache_entry, in_msg.addr) == State:NP);
                trigger(Event:AllocateLine_OnInstall,in_msg.addr,
                        cache_entry,tbe);
              }
              //Else Trigger Replacement to Make Space
              else {                        
                assert(getState(tbe, cache_entry, in_msg.addr) == State:NP);
                Addr victim_addr :=  L1Dcache.cacheProbe(in_msg.addr,true);
                if(L1Dcache.isAddrValid(victim_addr)){
                  trigger(Event:L1_Replacement_OnInstall,
                          victim_addr,
                          getL1DCacheEntry(victim_addr),
                          TBEs[victim_addr]);
                }  else {
                  DPRINTF(RubySlicc, "DATA:for Addr: %#x in State: %s. Victim :%#x Not VALID! Recycle\n", 
                        in_msg.addr,getState(tbe, cache_entry, in_msg.addr),victim_addr);
                  trigger(Event:L1_Replacement_VictimBusy, in_msg.addr, cache_entry,tbe);             

                }
              }
            }            

          } else if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            
            if(is_valid(cache_entry)){
              set_max_depth(tbe,in_msg.max_depth);
              trigger(Event:Data_all_Acks, in_msg.addr, cache_entry, tbe);
            } else {
              //Cache Has Space Avail, Then Allot Cache Entry & Reexecute.
              if (L1Dcache.cacheAvail(in_msg.addr)) {
                if(getState(tbe, cache_entry, in_msg.addr) != State:NP){
                    DPRINTF(RubySlicc, "AllocateLineOnInstall:for Addr: %#x in State: %s. IsValid_CacheEntry: %d, Line not in StateNP? ASSERT FAILS!\n", 
                          in_msg.addr,getState(tbe, cache_entry, in_msg.addr),is_valid(cache_entry));
                
                }
                assert(getState(tbe, cache_entry, in_msg.addr) == State:NP);
                trigger(Event:AllocateLine_OnInstall,in_msg.addr,
                        cache_entry,tbe);
              }
              //Else Trigger Replacement to Make Space
              else {                        
                assert(getState(tbe, cache_entry, in_msg.addr) == State:NP);
                Addr victim_addr :=  L1Dcache.cacheProbe(in_msg.addr,true);
                if(L1Dcache.isAddrValid(victim_addr)){
                  trigger(Event:L1_Replacement_OnInstall,
                          victim_addr,
                          getL1DCacheEntry(victim_addr),
                          TBEs[victim_addr]);
                }  else {
                  DPRINTF(RubySlicc, "Data AllAcksDone:for Addr: %#x in State: %s. Victim :%#x Not VALID! Recycle\n", 
                        in_msg.addr,getState(tbe, cache_entry, in_msg.addr),victim_addr);
                  trigger(Event:L1_Replacement_VictimBusy, in_msg.addr, cache_entry,tbe);             

                }
              }
            }
            
          } else {
            set_max_depth(tbe,in_msg.max_depth);
            trigger(Event:Data, in_msg.addr, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:ACK) {
          if ( (getPendingAcks(tbe) - in_msg.AckCount) == 0 ) {
            trigger(Event:Ack_all, in_msg.addr, cache_entry, tbe);
          } else {
            trigger(Event:Ack, in_msg.addr, cache_entry, tbe);
          }
        } else if (in_msg.Type == CoherenceResponseType:WB_ACK) {
          DPRINTF(RubySlicc, "WB_Ack Response triggered for Addr: %#x in State: %s\n",
                  in_msg.addr,getState(tbe, cache_entry, in_msg.addr));
          trigger(Event:WB_Ack, in_msg.addr, cache_entry, tbe);
        } else {
          error("Invalid L1 response type");
        }
      }
    }
  }

  // Request InterChip network - request from this L1 cache to the shared L2
  in_port(requestL1Network_in, RequestMsg, requestToL1Cache, rank = 1) {
    if(requestL1Network_in.isReady(clockEdge())) {
      peek(requestL1Network_in, RequestMsg, block_on="addr") {
        assert(in_msg.Destination.isElement(machineID));

        Entry cache_entry := getCacheEntry(in_msg.addr);
        TBE tbe := TBEs[in_msg.addr];

        if (in_msg.Type == CoherenceRequestType:INV) {
          trigger(Event:Inv, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GETX ||
                   in_msg.Type == CoherenceRequestType:UPGRADE) {
          // upgrade transforms to GETX due to race
          trigger(Event:Fwd_GETX, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GETS) {
          trigger(Event:Fwd_GETS, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GET_INSTR) {
          trigger(Event:Fwd_GET_INSTR, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:GETSPEC) {
          trigger(Event:Fwd_GETSPEC, in_msg.addr, cache_entry, tbe);
        } else if (in_msg.Type == CoherenceRequestType:EXPOSE) {
          trigger(Event:Fwd_EXPOSE, in_msg.addr, cache_entry, tbe);
        } else {
          error("Invalid forwarded request type");
        }
      }
    }
  }

  // Mandatory Queue betweens Node's CPU and it's L1 caches
  in_port(mandatoryQueue_in, RubyRequest, mandatoryQueue, desc="...", rank = 0) {
    if (mandatoryQueue_in.isReady(clockEdge())) {
      peek(mandatoryQueue_in, RubyRequest, block_on="LineAddress") {

        DPRINTF(RubySlicc,"Mandatory  Queue has Addr:%#x of MsgType:%s\n",in_msg.LineAddress,in_msg.Type);  


        // Check for data access to blocks in I-cache and ifetchs to blocks in D-cache
        if (in_msg.Type == RubyRequestType:IFETCH) {
          // ** INSTRUCTION ACCESS ***
          Entry L1Icache_entry := getL1ICacheEntry(in_msg.LineAddress);
          if (is_valid(L1Icache_entry)) {
          DPRINTF(RubySlicc, "**Inst Access (hit) for Addr: %#x\n",
                  in_msg.LineAddress);
            // The tag matches for the L1, so the L1 asks the L2 for it.
            trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                    L1Icache_entry, TBEs[in_msg.LineAddress]);
          } else {

            // Check to see if it is in the OTHER L1
            Entry L1Dcache_entry := getL1DCacheEntry(in_msg.LineAddress);
            if (is_valid(L1Dcache_entry)) {
              DPRINTF(RubySlicc, "**Inst Access (hit in L1D) for Addr: %#x. Repl initiated.\n",
                  in_msg.LineAddress);

              // The block is in the wrong L1, put the request on the queue to the shared L2
              trigger(Event:L1_Replacement, in_msg.LineAddress,
                      L1Dcache_entry, TBEs[in_msg.LineAddress]);
            }

            if (L1Icache.cacheAvail(in_msg.LineAddress)) {
              // L1 does't have the line, but we have space for it
              // in the L1 so let's see if the L2 has it.
              DPRINTF(RubySlicc, "**Inst Access (miss in L1I) for Addr: %#x. Has Space:Install.\n",
                  in_msg.LineAddress);

              trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                      L1Icache_entry, TBEs[in_msg.LineAddress]);
            } else {
              // No room in the L1, so we need to make room in the L1
              Addr victim_addr := L1Icache.cacheProbe(in_msg.LineAddress,false);
              DPRINTF(RubySlicc, "**Inst Access (miss in L1I) for Addr: %#x. Repl Init for Victim:%#x.\n",
                      in_msg.LineAddress, victim_addr);
              trigger(Event:L1_Replacement, victim_addr,
                      getL1ICacheEntry(victim_addr),
                      TBEs[victim_addr]);
            }
          }
        } else if(in_msg.Type == RubyRequestType:CLEANUP_EXECUTED ){

          DPRINTF(RubySlicc, "**CleanupExec Req  for Addr: %#x\n",
                  in_msg.LineAddress);

          //Check if there is Pending cleanup to the set before this one.
          if( (L1Dcache.cleanup_issue_pending()) || 
              ( L1Dcache.cleanup_prev_to_set(in_msg.LineAddress,in_msg.ct_id) && (!L1Dcache.cleanup_done(in_msg.LineAddress,in_msg.ct_id)) )) {
                
            DPRINTF(RubySlicc,"Cleanup Pending for Addr:%#x.  Waiting for prev cleanup to set to finish \n",in_msg.LineAddress);  
            trigger(Event:Waiting_CLEANUP,in_msg.LineAddress,  
                    getL1DCacheEntry(in_msg.LineAddress), TBEs[in_msg.LineAddress]);                      
            //Evict Installed line.
          } else if(L1Dcache.cleanup_needs_evictL1(in_msg.LineAddress,in_msg.ct_id)){
            DPRINTF(RubySlicc, "**CleanuprReq for Addr: %#x, Attempting L1-Invalidation of Addr: %#x\n",
                    in_msg.LineAddress, in_msg.LineAddress);

            Entry entry_toevict := getL1DCacheEntry(in_msg.LineAddress);
            trigger(Event:Inv_CLEANUP,in_msg.LineAddress,           
                    entry_toevict, TBEs[in_msg.LineAddress]);
            
            //Rollback Original Line
          } else if (L1Dcache.cleanup_needs_rollback(in_msg.LineAddress,in_msg.ct_id)){
            Addr install_lineaddr := L1Dcache.cleanup_get_rollback_addr(in_msg.LineAddress,in_msg.ct_id);
            Entry entry_toinstall := getL1DCacheEntry(install_lineaddr);
            DPRINTF(RubySlicc, "**CleanuprReq for Addr: %#x, Attempting L1-Rollback of Addr: %#x\n",
                    in_msg.LineAddress, install_lineaddr);            
            
            if(L1Dcache.cleanup_inflight_received(install_lineaddr)){
                DPRINTF(RubySlicc, "**CleanuprReq for Addr: %#x Reycled, Rollback Addr: %#x is Undergoing Cleanuo\n",
                    in_msg.LineAddress, install_lineaddr);
            
                trigger(Event:Waiting_Rollback_CLEANUP,in_msg.LineAddress,
                        entry_toinstall, TBEs[install_lineaddr]); 
              }

            if(L1Icache.isTagPresent(install_lineaddr)){
              //L1-Icache installed this line, would have anyway invalidated copy in D-Cache to avoid duplicates
              trigger(Event:Skip_ROLLBACK,install_lineaddr,     //So Skip Rollback
                      entry_toinstall, TBEs[install_lineaddr]);
              
            }  else if(is_valid(entry_toinstall) || L1Dcache.cacheAvail(install_lineaddr)){
              //Space available or line itself to be rolled back present.
              trigger(Event:Load_ROLLBACK,install_lineaddr,     
                      entry_toinstall, TBEs[install_lineaddr]); 

            } else if( is_invalid(entry_toinstall)
                       && (!L1Dcache.cacheAvail(install_lineaddr))
                       && (!L1Dcache.cleanup_prev_to_set(in_msg.LineAddress,in_msg.ct_id)) ){
              //Space not available and no other pending conflicting cleanup request to same set (which may rollback)
              Addr victim_addr := L1Dcache.cacheProbe(install_lineaddr,false);
              DPRINTF(RubySlicc, "**Space unavailable for CleanuprReq Addr: %#x. Replacing Addr %#x for L1-Rollback of Addr: %#x\n",
                      in_msg.LineAddress, victim_addr, install_lineaddr);
              
              trigger(Event:L1_Replacement,victim_addr,    
                      getL1DCacheEntry(victim_addr), TBEs[victim_addr]);              
            } else {              
              //Wait
              trigger(Event:Waiting_Rollback_CLEANUP,in_msg.LineAddress,
                      entry_toinstall, TBEs[install_lineaddr]); 
            }
            
            //Trigger Eviction from L2 for LineAddress if required.            
          } else if(L1Dcache.cleanup_needs_evictL2(in_msg.LineAddress,in_msg.ct_id)){
            Entry L1Dentry := getL1DCacheEntry(in_msg.LineAddress);
            DPRINTF(RubySlicc, "**Triggering PUT_INV req to L2 for Addr: %#x\n",in_msg.LineAddress);
            trigger(Event:InvL2_CLEANUP,in_msg.LineAddress,
                    L1Dentry, TBEs[in_msg.LineAddress]);

            //Check if Eviction Done?
          } else if(L1Dcache.cleanup_done(in_msg.LineAddress,in_msg.ct_id)){
            Entry L1Dentry := getL1DCacheEntry(in_msg.LineAddress);
            trigger(Event:Done_CLEANUP,in_msg.LineAddress,         
                    L1Dentry, TBEs[in_msg.LineAddress]);   

          } else {
            //Wait
            Entry L1Dentry := getL1DCacheEntry(in_msg.LineAddress);
            trigger(Event:Waiting_Final_CLEANUP,in_msg.LineAddress, 
                    L1Dentry, TBEs[in_msg.LineAddress]); 
          }
        }

        else if(in_msg.Type == RubyRequestType:CLEANUP_INFLIGHT ){

          if(TBEs.isPresent(in_msg.LineAddress)){
          DPRINTF(RubySlicc, "**CleanupInflight Req  for Addr: %#x. LineInL2:%d\n",
                  in_msg.LineAddress,TBEs[in_msg.LineAddress].lineInL2);
            trigger(Event:Inflight_CLEANUP_reqs,in_msg.LineAddress,           
                    getL1DCacheEntry(in_msg.LineAddress), TBEs[in_msg.LineAddress]);                      
          } else {
            DPRINTF(RubySlicc, "**Skipping CleanupInflight Req for Addr: %#x, TBEisPresent:%d, ReadReqTablePresent:%d. Issuing Ack Directly\n",
                    in_msg.LineAddress,TBEs.isPresent(in_msg.LineAddress),
                    sequencer.checkPresentReadReqTable(in_msg.LineAddress));

            trigger(Event:Skip_Inflight_Cleanup,in_msg.LineAddress,           
                    getL1DCacheEntry(in_msg.LineAddress), TBEs[in_msg.LineAddress]);                      
          }
          
        }

        else {
         
          DPRINTF(RubySlicc, "**Received Data Req for Addr: %#x\n",in_msg.LineAddress);

          // *** DATA ACCESS ***
          Entry L1Dcache_entry := getL1DCacheEntry(in_msg.LineAddress);

          // If Cleanup Inflight Request Received for Load, then Drop it right away.
          if( (!sequencer.checkPresentReadReqTable(in_msg.LineAddress))
              && (in_msg.Type == RubyRequestType:LD) ){

             DPRINTF(RubySlicc,"Skipping any cleanup for Addr:%#x CleanupInflight: Dropping Load Request\n",in_msg.LineAddress);
            trigger(Event:Drop_Load_Cleanup_Inflight, in_msg.LineAddress,
                    L1Dcache_entry, TBEs[in_msg.LineAddress]);            
          }

          if( (L1Dcache.cleanup_table_size() > 0) &&
              (!( L1Dcache.cleanup_issue_pending() && sequencer.checkDataCleanupDependancy(in_msg.LineAddress) )) ){
            //Wait for Cleanup to Complete,
            //Unless there is a issue of cleanup req inflight pending, because of a data req inflight pending.
            
            DPRINTF(RubySlicc, "**For Data Addr:%#x, Need to WAIT while cleanup finishes. Current CT-size: %d \n",in_msg.LineAddress, L1Dcache.cleanup_table_size());
            trigger(Event:Waiting_CLEANUP_inprogress, in_msg.LineAddress,
                    L1Dcache_entry, TBEs[in_msg.LineAddress]);
          }
          
          if (is_valid(L1Dcache_entry)) {
            // The tag matches for the L1, so the L1 doesnt ask the L2 for it
            DPRINTF(RubySlicc, "**For Data Addr:%#x, TAG-MATCH. State:%s \n",in_msg.LineAddress,getState(TBEs[in_msg.LineAddress], L1Dcache_entry, in_msg.LineAddress));
            trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                    L1Dcache_entry, TBEs[in_msg.LineAddress]);
          } else {

            // Check to see if it is in the OTHER L1
            Entry L1Icache_entry := getL1ICacheEntry(in_msg.LineAddress);
            if (is_valid(L1Icache_entry)) {
              DPRINTF(RubySlicc, "**For Data Addr:%#x, TAG-MATCH-L1-INST \n",in_msg.LineAddress);
              // The block is in the wrong L1, put the request on the queue to the shared L2
              trigger(Event:L1_Replacement, in_msg.LineAddress,
                      L1Icache_entry, TBEs[in_msg.LineAddress]);
            }

            if (L1Dcache.cacheAvail(in_msg.LineAddress)) {
              // L1 does't have the line, but we have space for it
              // in the L1 let's see if the L2 has it.
              DPRINTF(RubySlicc, "**For Data Addr:%#x in State:%s, CACHEAVAIL=YES \n",in_msg.LineAddress,getState(TBEs[in_msg.LineAddress], L1Dcache_entry, in_msg.LineAddress));
              trigger(mandatory_request_type_to_event(in_msg.Type), in_msg.LineAddress,
                      L1Dcache_entry, TBEs[in_msg.LineAddress]);
            } else {
              if(in_msg.Type == RubyRequestType:LD) {
                //Check if there is a Race where Load in Progress But Another Load Comes.
                if(TBEs.isPresent(in_msg.LineAddress)){
                  if(getState(TBEs[in_msg.LineAddress], L1Dcache_entry, in_msg.LineAddress) == State:NP){
                     trigger(Event:Delay_Load_RaceCondition, in_msg.LineAddress,
                             L1Dcache_entry, TBEs[in_msg.LineAddress]);
}   
                }

                //Trigger Load, We will take care of Replacement later. 
                trigger(Event:Load, in_msg.LineAddress,
                        L1Dcache_entry, TBEs[in_msg.LineAddress]);
              } else {
                // No room in the L1, Trigger Replacement
                Addr victim_addr := L1Dcache.cacheProbe(in_msg.LineAddress,true);
                DPRINTF(RubySlicc, "**For Data Addr:%#x, CACHEPROBE->VICT_ADDR=%#x \n",in_msg.LineAddress,victim_addr);
                if(L1Dcache.isAddrValid(victim_addr)){
                  DPRINTF(RubySlicc, "**For Data Addr:%#x, REPL begins for VICT_ADDR=%#x \n",in_msg.LineAddress,victim_addr);
                  trigger(Event:L1_Replacement, victim_addr,
                          getL1DCacheEntry(victim_addr),
                          TBEs[victim_addr]);
                } else{
                  DPRINTF(RubySlicc, "**For Data Addr:%#x, REPL Stalled till parallel REPL inprogress completes\n",in_msg.LineAddress);
                  trigger(Event:Waiting_REPL_inprogress, in_msg.LineAddress,
                          L1Dcache_entry, TBEs[in_msg.LineAddress]);
                }
              }
            }
          }
        }
      }
    }
  }

  void enqueuePrefetch(Addr address, RubyRequestType type) {
    enqueue(optionalQueue_out, RubyRequest, 1) {
    out_msg.LineAddress := address;
    out_msg.Type := type;
    out_msg.AccessMode := RubyAccessMode:Supervisor;
    }
  }

  // ACTIONS
  action(a_issueGETS, "a", desc="Issue GETS") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:GETS;
      out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      out_msg.idx := in_msg.idx;
      }
    }
  }

  action(as_issueGETSPEC, "as", desc="Issue GETSPEC") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:GETSPEC;
      out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:SPECLD_Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      out_msg.idx := in_msg.idx;
      }
    }
  }

  action(ex_issueEXPOSE, "ex", desc="Issue EXPOSE") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:EXPOSE;
      out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:EXPOSE_Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      out_msg.idx := in_msg.idx;
      }
    }
  }

  action(pa_issuePfGETS, "pa", desc="Issue prefetch GETS") {
    peek(optionalQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:GETS;
      out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      }
    }
  }

  action(ai_issueGETINSTR, "ai", desc="Issue GETINSTR") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:GET_INSTR;
      out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      out_msg.idx := in_msg.idx;
      }
    }
  }

  action(pai_issuePfGETINSTR, "pai",
         desc="Issue GETINSTR for prefetch request") {
    peek(optionalQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:GET_INSTR;
      out_msg.Requestor := machineID;
        out_msg.Destination.add(
                                mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
      out_msg.MessageSize := MessageSizeType:Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;

        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      }
    }
  }

  action(b_issueGETX, "b", desc="Issue GETX") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:GETX;
      out_msg.Requestor := machineID;
        DPRINTF(RubySlicc, "%s\n", machineID);
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      out_msg.idx := in_msg.idx;
      }
    }
  }

  action(pb_issuePfGETX, "pb", desc="Issue prefetch GETX") {
    peek(optionalQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg, l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:GETX;
      out_msg.Requestor := machineID;
        DPRINTF(RubySlicc, "%s\n", machineID);

        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));

        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      }
    }
  }

  action(c_issueUPGRADE, "c", desc="Issue GETX") {
    peek(mandatoryQueue_in, RubyRequest) {
      enqueue(requestL1Network_out, RequestMsg,  l1_request_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:UPGRADE;
      out_msg.Requestor := machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        DPRINTF(RubySlicc, "address: %#x, destination: %s\n",
                address, out_msg.Destination);
      out_msg.MessageSize := MessageSizeType:Control;
      out_msg.Prefetch := in_msg.Prefetch;
      out_msg.AccessMode := in_msg.AccessMode;
      out_msg.idx := in_msg.idx;
      }
    }
  }

  action(d_sendDataToRequestor, "d", desc="send data to requestor") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "d: **Sending ResponseType:DATA Address: %#x, Destination: %s, State:%s \n",
              out_msg.addr, out_msg.Destination,getState(tbe, cache_entry, out_msg.addr));

      }
    }
  }

  action(dex_sendDataToExposeRequestor, "dex", desc="send data to requestor") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:EXPOSE_Data;
      DPRINTF(RubySlicc, "dex: **Sending ResponseType:DATA Address: %#x, Destination: %s, State:%s \n",
              out_msg.addr, out_msg.Destination,getState(tbe, cache_entry, out_msg.addr));

      }
    }
  }

  action(ds_sendDataToSpecRequestor, "ds", desc="send data to requestor") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(cache_entry));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := cache_entry.DataBlk;
      out_msg.Dirty := cache_entry.Dirty;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:SPECLD_Data;
      }
    }
  }

  action(d2_sendDataToL2, "d2", desc="send data to the L2 cache because of M downgrade") {
    enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:DATA;
    out_msg.DataBlk := cache_entry.DataBlk;
    out_msg.Dirty := cache_entry.Dirty;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:Response_Data;
    DPRINTF(RubySlicc, "d2: **Sending ResponseType:DATA Address: %#x, Destination: %s, State:%s \n",
              out_msg.addr, out_msg.Destination,getState(tbe, cache_entry, out_msg.addr));

    }
  }

  action(d2ex_sendExposeDataToL2, "d2ex", desc="send data to the L2 cache because of M downgrade") {
    enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:DATA;
    out_msg.DataBlk := cache_entry.DataBlk;
    out_msg.Dirty := cache_entry.Dirty;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:EXPOSE_Data;
    }
  }

  action(dt_sendDataToRequestor_fromTBE, "dt", desc="send data to requestor") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(tbe));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:Response_Data;
      DPRINTF(RubySlicc, "dt: **Sending ResponseType:DATA Address: %#x, Destination: %s, State:%s \n",
              out_msg.addr, out_msg.Destination,getState(tbe, cache_entry, out_msg.addr));

      }
    }
  }

  action(dtex_sendDataToExposeRequestor_fromTBE, "dtex", desc="send data to requestor") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(tbe));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:EXPOSE_Data;
      }
    }
  }

  action(dts_sendDataToSpecRequestor_fromTBE, "dts", desc="send data to requestor") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
        assert(is_valid(tbe));
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:DATA;
      out_msg.DataBlk := tbe.DataBlk;
      out_msg.Dirty := tbe.Dirty;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:SPECLD_Data;
      }
    }
  }

  action(d2t_sendDataToL2_fromTBE, "d2t", desc="send data to the L2 cache") {
    enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(tbe));
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:DATA;
    out_msg.DataBlk := tbe.DataBlk;
    out_msg.Dirty := tbe.Dirty;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:Response_Data;
        DPRINTF(RubySlicc, "d2t: **Sending ResponseType:DATA Address: %#x, Destination: %s, State:%s \n",
              out_msg.addr, out_msg.Destination,getState(tbe, cache_entry, out_msg.addr));

    }
  }

  action(d2tex_sendExposeDataToL2_fromTBE, "d2tex", desc="send data to the L2 cache") {
    enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(tbe));
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:DATA;
    out_msg.DataBlk := tbe.DataBlk;
    out_msg.Dirty := tbe.Dirty;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:EXPOSE_Data;
    }
  }

  action(e_sendAckToRequestor, "e", desc="send invalidate ack to requestor (could be L2 or L1)") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:ACK;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:Response_Control;
      }
    }
  }

  action(f_sendDataToL2, "f", desc="send data to the L2 cache") {
    enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:DATA;
    out_msg.DataBlk := cache_entry.DataBlk;
    out_msg.Dirty := cache_entry.Dirty;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:Writeback_Data;
    DPRINTF(RubySlicc, "f: **Sending ResponseType:DATA Address: %#x, Destination: %s, State:%s \n",
              out_msg.addr, out_msg.Destination,getState(tbe, cache_entry, out_msg.addr));

    }
  }

  action(ft_sendDataToL2_fromTBE, "ft", desc="send data to the L2 cache") {
    enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      assert(is_valid(tbe));
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:DATA;
    out_msg.DataBlk := tbe.DataBlk;
    out_msg.Dirty := tbe.Dirty;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:Writeback_Data;
    DPRINTF(RubySlicc, "ft: **Sending ResponseType:DATA Address: %#x, Destination: %s, State:%s \n",
              out_msg.addr, out_msg.Destination,getState(tbe, cache_entry, out_msg.addr));

    }
  }

  action(fi_sendInvAck, "fi", desc="send data to the L2 cache") {
    peek(requestL1Network_in, RequestMsg) {
      enqueue(responseL1Network_out, ResponseMsg, l1_response_latency) {
      out_msg.addr := address;
      out_msg.Type := CoherenceResponseType:ACK;
      out_msg.Sender := machineID;
        out_msg.Destination.add(in_msg.Requestor);
      out_msg.MessageSize := MessageSizeType:Response_Control;
      out_msg.AckCount := 1;
      }
    }
  }

  action(forward_eviction_to_cpu, "\cc", desc="sends eviction information to the processor") {
    if (send_evictions) {
      DPRINTF(RubySlicc, "Sending invalidation for %#x to the CPU\n", address);
      sequencer.evictionCallback(address, false);
    }
  }

  action(forward_external_eviction_to_cpu, "\ccc", desc="sends external eviction information to the processor") {
    if (send_evictions) {
      DPRINTF(RubySlicc, "Sending invalidation for %#x to the CPU\n", address);
      sequencer.evictionCallback(address, true);
    }
  }

  action(g_issuePUTX, "g", desc="send data to the L2 cache") {
    enqueue(requestL1Network_out, RequestMsg, l1_response_latency) {
      assert(is_valid(cache_entry));
    out_msg.addr := address;
    out_msg.Type := CoherenceRequestType:PUTX;
    out_msg.DataBlk := cache_entry.DataBlk;
    out_msg.Dirty := cache_entry.Dirty;
    out_msg.Requestor:= machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
      if (cache_entry.Dirty) {
      out_msg.MessageSize := MessageSizeType:Writeback_Data;
      } else {
      out_msg.MessageSize := MessageSizeType:Writeback_Control;
      }
    }
  }

  action(j_sendUnblock, "j", desc="send unblock to the L2 cache") {
    enqueue(unblockNetwork_out, ResponseMsg, to_l2_latency) {
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:UNBLOCK;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:Response_Control;
      DPRINTF(RubySlicc, "Transition to %s state complete: SentUnblock to Directory for %#x\n",getState(tbe, cache_entry, address), address);
    }
  }

  action(jj_sendExclusiveUnblock, "\j", desc="send unblock to the L2 cache") {
    enqueue(unblockNetwork_out, ResponseMsg, to_l2_latency) {
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:EXCLUSIVE_UNBLOCK;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:Response_Control;
      DPRINTF(RubySlicc, "Transition to %s state complete: SentUnblock to Directory for %#x\n",getState(tbe, cache_entry, address), address);

    }
  }


  action(jjr_sendExclusiveUnblockRevert, "jjr", desc="send unblock to the L2 cache and ask it to revert its state") {
    enqueue(unblockNetwork_out, ResponseMsg, to_l2_latency) {
    out_msg.addr := address;
    out_msg.Type := CoherenceResponseType:EXCLUSIVE_UNBLOCK_REVERT;
    out_msg.Sender := machineID;
      out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                l2_select_low_bit, l2_select_num_bits, intToID(0)));
    out_msg.MessageSize := MessageSizeType:Response_Control;
      DPRINTF(RubySlicc, "Transition to %s state complete: SentUnblockRevert to Directory for %#x\n",getState(tbe, cache_entry, address), address);

    }
  }

  action(dg_invalidate_sc, "dg",
         desc="Invalidate store conditional as the cache lost permissions") {
    sequencer.invalidateSC(address);
  }

  action(h_load_hit, "hd",
         desc="Notify sequencer the load completed.")
    {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
      L1Dcache.setMRU(cache_entry);
      sequencer.readCallback(address, cache_entry.DataBlk);
    
    }

  action(h_spec_load_hit, "hs",
         desc="Notify sequencer the spec load completed.")
    {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
      sequencer.readCallback(address, cache_entry.DataBlk);
    }

  action(h_ifetch_hit, "hi", desc="Notify sequencer the instruction fetch completed.")
    {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
      L1Icache.setMRU(cache_entry);
      sequencer.readCallback(address, cache_entry.DataBlk);
    }

  action(hx_load_hit, "hx", desc="Notify sequencer the load completed.")
    {
      if(!tbe.Rollback){ //[CleanupCache]
        assert(is_valid(cache_entry));
        DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
        L1Icache.setMRU(address);
        L1Dcache.setMRU(address);
        bool L2Hit := false;
        if(get_max_depth(tbe) == 2){
        L2Hit := true;
        }
        //L1Dcache.removeMSHRentry(address);
        sequencer.readCallback(address, cache_entry.DataBlk, true, L2Hit);
      }
    }

  action(hsx_spec_load_hit, "hsx", desc="Notify sequencer the external load completed.")
    {
      peek(responseL1Network_in, ResponseMsg) {
        // [InvisiSpec] Hack for in_msg.DataBlk returning const DataBlk
      tbe.DataBlk := in_msg.DataBlk;
        bool L2Hit := false;
        if(get_max_depth(tbe) == 2){
        L2Hit := true;
        }
        //L1Dcache.removeMSHRentry(address);      
        sequencer.readCallback(address, tbe.DataBlk, true, L2Hit);
      }
    }

  action(hh_store_hit, "\h", desc="Notify sequencer that store completed.")
    {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
      L1Dcache.setMRU(cache_entry);
      sequencer.writeCallback(address, cache_entry.DataBlk);
    cache_entry.Dirty := true;
    }

  action(hhx_store_hit, "\hx", desc="Notify sequencer that store completed.")
    {
      assert(is_valid(cache_entry));
      DPRINTF(RubySlicc, "%s\n", cache_entry.DataBlk);
      L1Icache.setMRU(address);
      L1Dcache.setMRU(address);
      //L1Dcache.removeMSHRentry(address);
      sequencer.writeCallback(address, cache_entry.DataBlk, true);
    cache_entry.Dirty := true;
    }

  action(i_allocateTBE, "i", desc="Allocate TBE (isPrefetch=0, number of invalidates=0)") {
    check_allocate(TBEs);
    assert(is_valid(cache_entry));    
    /*
    if(TBEs.isPresent(address)){
      TBEs.deallocate(address);
      unset_tbe();
    }
    */
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.isPrefetch := false;
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
  tbe.lineInL2 := L2cache.isTagPresent(address);  // Like an Oracle for L2 Cleanup (to prevent Data-Races)
  }


  action(iw_allocateTBEWithoutCacheEntry, "iw", desc="Allocate TBE without a cache entry") {
    check_allocate(TBEs);
    assert(!is_valid(cache_entry));
    if(TBEs.isPresent(address)){
      TBEs.deallocate(address);
      unset_tbe();
    }
    TBEs.allocate(address);
    set_tbe(TBEs[address]);
    tbe.isPrefetch := false;
    tbe.Rollback := false;
    tbe.lineInL2 := L2cache.isTagPresent(address); // Like an Oracle for L2 Cleanup (to prevent Data-Races)
  }

  action(utoi_updateTBEOnInstall, "utoi", desc="Update TBE when cache-entry is created on Install") {    
    assert(is_valid(cache_entry));
    tbe.isPrefetch := false;
    tbe.Rollback := false;
    tbe.Dirty := cache_entry.Dirty;
    tbe.DataBlk := cache_entry.DataBlk;
    //**TODO**: Bit to store whether cleanup-issued for this install?
  }

  action(k_popMandatoryQueue, "k", desc="Pop mandatory queue.") {
    mandatoryQueue_in.dequeue(clockEdge());
  }

  action(l_popRequestQueue, "l",
         desc="Pop incoming request queue and profile the delay within this virtual network") {
    Tick delay := requestL1Network_in.dequeue(clockEdge());
    profileMsgDelay(2, ticksToCycles(delay));
  }

  action(o_popIncomingResponseQueue, "o",
         desc="Pop Incoming Response queue and profile the delay within this virtual network") {
    Tick delay := responseL1Network_in.dequeue(clockEdge());
    profileMsgDelay(1, ticksToCycles(delay));
  }

  action(s_deallocateTBE, "s", desc="Deallocate TBE") {
    TBEs.deallocate(address);
    unset_tbe();
  }

  action(u_writeDataToL1Cache, "u", desc="Write data to cache") {
    peek(responseL1Network_in, ResponseMsg) {
      assert(is_valid(cache_entry));
    cache_entry.DataBlk := in_msg.DataBlk;
    cache_entry.Dirty := in_msg.Dirty;
    }
  }

  action(q_updateAckCount, "q", desc="Update ack count") {
    peek(responseL1Network_in, ResponseMsg) {
      assert(is_valid(tbe));
    tbe.pendingAcks := tbe.pendingAcks - in_msg.AckCount;
      APPEND_TRANSITION_COMMENT(in_msg.AckCount);
      APPEND_TRANSITION_COMMENT(" p: ");
      APPEND_TRANSITION_COMMENT(tbe.pendingAcks);
    }
  }

  action(ff_deallocateL1CacheBlock, "\f", desc="Deallocate L1 cache block.  Sets the cache to not present, allowing a replacement in parallel with a fetch.") {
    if (L1Dcache.isTagPresent(address)) {
      L1Dcache.deallocate(address);
    } else {
      L1Icache.deallocate(address);
    }
    unset_cache_entry();
  }

  action(oo_allocateL1DCacheBlock, "\o", desc="Set L1 D-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(L1Dcache.allocate(address, new Entry));
    }
  }

  action(pp_allocateL1ICacheBlock, "\p", desc="Set L1 I-cache tag equal to tag of block B.") {
    if (is_invalid(cache_entry)) {
      set_cache_entry(L1Icache.allocate(address, new Entry));
    }
  }

  action(z_stallAndWaitMandatoryQueue, "\z", desc="recycle L1 request queue") {
    stall_and_wait(mandatoryQueue_in, address);
  }

  action(kd_wakeUpDependents, "kd", desc="wake-up dependents") {
    wakeUpBuffers(address);
  }

  action(uu_profileInstMiss, "\uim", desc="Profile the demand miss") {
    ++L1Icache.demand_misses;
  }

  action(uu_profileInstHit, "\uih", desc="Profile the demand hit") {
    ++L1Icache.demand_hits;
  }

  action(uu_profileDataMiss, "\udm", desc="Profile the demand miss") {
    ++L1Dcache.demand_misses;
  }

  action(uu_profileDataHit, "\udh", desc="Profile the demand hit") {
    ++L1Dcache.demand_hits;
  }

  action(po_observeMiss, "\po", desc="Inform the prefetcher about the miss") {
    peek(mandatoryQueue_in, RubyRequest) {
      if (enable_prefetch) {
        prefetcher.observeMiss(in_msg.LineAddress, in_msg.Type);
      }
    }
  }

  action(ppm_observePfMiss, "\ppm",
         desc="Inform the prefetcher about the partial miss") {
    peek(mandatoryQueue_in, RubyRequest) {
      prefetcher.observePfMiss(in_msg.LineAddress);
    }
  }

  action(pq_popPrefetchQueue, "\pq", desc="Pop the prefetch request queue") {
    optionalQueue_in.dequeue(clockEdge());
  }

  action(mp_markPrefetched, "mp", desc="Write data from response queue to cache") {
    assert(is_valid(cache_entry));
  cache_entry.isPrefetch := true;
  }

  action(x_expose_done, "xd",
         desc="Notify sequencer the expose completed.")
    {
      sequencer.readCallback(address, cache_entry.DataBlk);
    }


  //***********************************
  // CLEANUP ACTIONS
  //***********************************

  //Recycle
  action(zzm_recycleMandatoryQueue, "zzm", desc="Send the head of the mandatory queue to the back of the queue.")
    {
      mandatoryQueue_in.recycle(clockEdge(), cyclesToTicks(recycle_latency));
    }

  action(zzr_recycleResponseQueue, "zzr", desc="Send the head of the response queue to the back of the queue.")
    {
      responseL1Network_in.recycle(clockEdge(), cyclesToTicks(recycle_latency));
    }

  //Error
  action(suc_screwed_up_cleanup, "suc",
         desc="Dummy Function called when something Bad Happens.")
    {
      DPRINTF(RubySlicc, "**We screwed up Cleanup. Debug for Addr:%#x! \n",address);
      assert(false);
      
    }

  //--------------------------------------------------------------------------------
  //Actions for Invalidations in L1 Cache

  action(cls_cleanup_L1inv_success, "cls",
         desc="Successful Invalidate on Cleanup")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**Successful Cleanup L1-Invalidate of Installed Line %#x ct_id:%d in state %s\n",address,in_msg.ct_id, getState(tbe, cache_entry, address));
              
        L1Dcache.cleanup_set_toEvictL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneEvictL1(address, true, in_msg.ct_id);
      }
    }

  action(clw_cleanup_L1inv_waiting, "clw",
         desc="Waiting on Triggers for Cleanup")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**Waiting on Intermediate States for Cleanup-L1 Cache Invalidation of Addr: %#x Ct_id:%d\n",address,in_msg.ct_id);      
      }
    }
  
  action(clinvf_cleanup_L1inv_fired, "clinvf",
         desc="L1 Invalidate Fired")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**L1 Invalidate Fired for %#x, %d. Waiting for Ack\n",address,in_msg.ct_id);
        L1Dcache.cleanup_set_toEvictL1(address, false, in_msg.ct_id);
      }
    }

  action(mlinvt_mark_L1inv_TBE, "mlinvt", desc="Mark L1Invalidated = True in TBE")
    {
      peek(mandatoryQueue_in, RubyRequest) {
      tbe.L1Invalidated := true;
      tbe.ct_id := in_msg.ct_id;
      }
    }

  action(dil_done_if_l1invalidate, "dil", desc="Marks a L1Invalidate request done, if TBE says it is such a request")
    {
      if(tbe.L1Invalidated){
        DPRINTF(RubySlicc, "**L1 Invalidate Done for %#x.\n",address,tbe.ct_id);
        L1Dcache.cleanup_set_doneEvictL1(address,true,tbe.ct_id);
      }
    }

  action(clirs2_cleanup_L12InvRoll_skip, "clirs2",
         desc="Skip Invalidate and Rollback on Cleanup")
    {
      peek(mandatoryQueue_in, RubyRequest) {
          DPRINTF(RubySlicc, "**Skipping Cleanup L1Invalidate and Rollback and L2 Invalidate for Addr %#x, Ct_id:%d in State %s\n",address, in_msg.ct_id, getState(tbe, cache_entry, address));
              
        L1Dcache.cleanup_set_toEvictL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneEvictL1(address, true, in_msg.ct_id);
        L1Dcache.cleanup_set_toInstallL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneInstallL1(address, true, in_msg.ct_id);
        L1Dcache.cleanup_set_toEvictL2(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneEvictL2(address, true, in_msg.ct_id);        
      }
    }

  action(clirs_cleanup_L1InvRoll_skip, "clirs",
         desc="Skip Invalidate and Rollback on Cleanup")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**Skipping Cleanup L1Invalidate and Rollback for Addr %#x, Ct_id:%d in State %s\n",address, in_msg.ct_id, getState(tbe, cache_entry, address));
              
        L1Dcache.cleanup_set_toEvictL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneEvictL1(address, true, in_msg.ct_id);
        L1Dcache.cleanup_set_toInstallL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneInstallL1(address, true, in_msg.ct_id);
      }
    }

  
  //--------------------------------------------------------------------------------
  //Actions for Rollback in L1 Cache
  action(clww_cleanup_L1rollback_waiting1, "clww",
         desc="Waiting on Space to be Available in Cache for L1 Cache rollback")
    {
      DPRINTF(RubySlicc, "**Waiting on Space to become available for L1 Cache Rollback of %#x\n",address);      
    }

  action(clrw_cleanup_L1rollback_waiting2, "clrw",
         desc="Waiting on Stable State for L1 Cache Line to be rolledback")
    {
      DPRINTF(RubySlicc, "**Waiting on Stable state to begin L1 Cache Rollback of %#x\n",address);
    }

  action(clf_cleanup_L1rollback_fired, "clf",
         desc="L1 Rollback Fired")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**L1 Rollback Fired for Addr:%#x, Ct_id:%d\n",address,in_msg.ct_id);
        Addr cleanup_addr :=  L1Dcache.cleanup_get_cleanup_addr(address,in_msg.ct_id);
        L1Dcache.cleanup_set_toInstallL1(cleanup_addr, false,in_msg.ct_id);
      }
    }
  
  action(cld_cleanup_L1rollback_done, "cld",
         desc="L1 Rollback Done")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**L1 Rollback Done for %#x, Ct_id:%d\n",address,in_msg.ct_id);
        Addr cleanup_addr := L1Dcache.cleanup_get_cleanup_addr(address,in_msg.ct_id);
        L1Dcache.cleanup_set_doneInstallL1(cleanup_addr, true,in_msg.ct_id);
      }
    }

  action(mrt_mark_rollback_TBE, "mrt", desc="Mark Rollback = True in TBE")
    {
      peek(mandatoryQueue_in, RubyRequest) {
      tbe.ct_id := in_msg.ct_id;
      tbe.Rollback := true;
      }
    }

  action(dir_done_if_rollback, "dir", desc="Marks a rollback request done, if TBE says it is such a request")
    {
      if(tbe.Rollback){
        Addr cleanup_addr := L1Dcache.cleanup_get_cleanup_addr(address,tbe.ct_id);
        L1Dcache.cleanup_set_doneInstallL1(cleanup_addr,true,tbe.ct_id);
      }
    }
    
  //--------------------------------------------------------------------------------
  //Actions for Invalidations in L2 Cache

//Allocate TBE when calling L2 Invalidation for L1 Exec Cleanup. If not already allocated
  action(iwac_allocateTBEWithoutCacheEntry_CleanupExec, "iwac", desc="Allocate TBE without a cache entry, if not already allocated") {
    check_allocate(TBEs);
    assert(!is_valid(cache_entry));
    if(!TBEs.isPresent(address)){
      TBEs.allocate(address);
      set_tbe(TBEs[address]);
    } else {
      peek(mandatoryQueue_in, RubyRequest) {
        assert(tbe.ct_id == in_msg.ct_id);
      }
    }      
    tbe.isPrefetch := false;
  }


  action(cclw_cleanup_L2invalidate_waiting, "cclw",
         desc="Waiting on NP State for line in L1 to be invalidated in L2")
    {
        DPRINTF(RubySlicc, "Waiting on NP State for line in L1 to be invalidated in L2 - Addr %#x, State: %s\n",address,getState(tbe, cache_entry, address));
    }

  action(ilf_invl2_fired,"ilf", //Mark toEvict_L2 = true; L2-invalidation started.
         desc="Mark toEvict_L2 = true; L2-invalixdation started.")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**L2 Invalidate Fired for Addr:%#x, Ct_id:%d\n",address,in_msg.ct_id);
        L1Dcache.cleanup_set_toEvictL2(address,false, in_msg.ct_id);
      }
    }  
  action(mtrl_markTBE_receive_L2invack,"mtrl",desc="Marks TBE to remember L2 Invalidation Ack is pending.") 
    {
      peek(mandatoryQueue_in, RubyRequest) {
      tbe.InvL2_AckReq := true;
      tbe.ct_id := in_msg.ct_id;
      }
    } 
   
  action(ipi_issuePUTinv,"ipi",desc="Send PUTinv packet to L2 and wait for ack")
    {
      enqueue(requestL1Network_out, RequestMsg, l1_response_latency) {

      out_msg.addr := address;
      out_msg.Type := CoherenceRequestType:PUTINV;
      out_msg.Requestor:= machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        out_msg.MessageSize := MessageSizeType:Response_Control;
      }

    }

  action(ild_invl2_done, "ild",
         desc="L2 Invalidate Done")
    {
      assert(tbe.InvL2_AckReq);
      DPRINTF(RubySlicc, "**L2 Invalidation Done for %#x, Ct_id:%d\n",address,tbe.ct_id);
      L1Dcache.cleanup_set_doneEvictL2(address, true,tbe.ct_id);
    }
  
  
  
  //TODO: Actions.

  //--------------------------------------------------------------------------------
  //Actions for Final Wait.
  action(cfw_cleanup_final_waiting, "cfw",
         desc="Waiting on All Issued Cleanup Activity to Complete")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**Waiting on All Issued Cleanup Activity to Complete for Addr:%#x, Ct_id:%d\n",address,in_msg.ct_id);      
      }
    }

  action(cd_cleanup_done, "cd", desc="Notify Sequencer that cleanup is done")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        sequencer.cleanupAckCallback(address,in_msg.ct_id);
      }
    }
  
  action(ciw_cleanup_inprogress_waiting, "ciw",
         desc="Waiting on Cleanup Inprogress to Complete")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**Data access for Addr: %#x, Ct_id:%d, Waiting on Cleanup Inprogress  to Complete\n",address,in_msg.ct_id);
      }
    }

  //--------------------------------------------------------------------------------
  //Actions for Repl Busy Wait.
  action(riw_repl_inprogress_waiting, "riw",
         desc="Waiting on Replacement Inprogress to Complete")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        DPRINTF(RubySlicc, "**Data install for Addr: %#x, Ct_id:%d Waiting on Replacement Inprogress to Complete\n",address,in_msg.ct_id);
      }
    }

  //--------------------------------------------------------------------------------
  //Actions for Inflight Cleanup

  action(tcis_tbe_cleanup_inflight_set, "tcis",
         desc="Update TBE State on Inflight Cleanup")
    {
      peek(mandatoryQueue_in, RubyRequest) {
        assert(L1Dcache.isTagPresent(address) == false);
        assert(is_valid(tbe));
        tbe.Inflight_cleanup_recvd := true;
        tbe.ct_id := in_msg.ct_id;
        DPRINTF(RubySlicc, "**Recorded CleanupInflight for Addr: %#x\n",address);

      }


    }

  action(cis_cleanup_inflight_stateupdate, "cis",
         desc="update state in CleanupTable")
    {
      peek(mandatoryQueue_in, RubyRequest) {              
        L1Dcache.cleanup_set_toEvictL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneEvictL1(address, true, in_msg.ct_id);
        L1Dcache.cleanup_set_toInstallL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneInstallL1(address, true, in_msg.ct_id);        
      }
    }

  action(icillc_issue_cleanup_inflight_l2, "icillc",
         desc="issue cleanupinflight request to L2")
    {

      //Issue inv, if line was installed in L2.
      if(tbe.lineInL2 == false){
        enqueue(requestL1Network_out, RequestMsg, l1_response_latency) {
        out_msg.addr := address;
        out_msg.Type := CoherenceRequestType:PUTINV;
        out_msg.Requestor:= machineID;
        out_msg.Destination.add(mapAddressToRange(address, MachineType:L2Cache,
                                                  l2_select_low_bit, l2_select_num_bits, intToID(0)));
        out_msg.MessageSize := MessageSizeType:Response_Control;
        }
        //EvictL2 not done.
        peek(mandatoryQueue_in, RubyRequest) {              
          L1Dcache.cleanup_set_toEvictL2(address,false, in_msg.ct_id);
          L1Dcache.cleanup_set_doneEvictL2(address,false, in_msg.ct_id);
        }
        //need to wait for InvL2 Response.
        tbe.InvL2_AckReq := true;  
      }
      else {
        if(tbe.received_L2ret_cleanup_inflight){

          //Already Received Unblock. EvictL2 done.
          peek(mandatoryQueue_in, RubyRequest) {              
            L1Dcache.cleanup_set_toEvictL2(address,false, in_msg.ct_id);
            L1Dcache.cleanup_set_doneEvictL2(address,true, in_msg.ct_id);
          } 

        } else {
        
          //Need to wait till Line Returns from L2. 
          tbe.need_wait_L2ret_cleanup_inflight := true;  
          tbe.received_L2ret_cleanup_inflight := false;
 
          peek(mandatoryQueue_in, RubyRequest) {              
            L1Dcache.cleanup_set_toEvictL2(address,false, in_msg.ct_id);
            L1Dcache.cleanup_set_doneEvictL2(address,false, in_msg.ct_id);
          }
        }
      }
    }

  action(mtlr_markTBEL2HitReceived, "mtlr", desc="Set in TBE that L2 Data Received completed, if exists") {
    if(TBEs.isPresent(address)){
       tbe.received_L2ret_cleanup_inflight := true;
    }
  }

  action(cia_cleanup_ack_ifwaitingonUnblock, "cia", desc="Process Cleanup Ack if L2Ret was the only critical wait.") {
    if(TBEs.isPresent(address)){

      if(tbe.need_wait_L2ret_cleanup_inflight 
         && tbe.received_L2ret_cleanup_inflight){
        
        //All Done - Can issue CleanupAck
        L1Dcache.cleanup_set_doneEvictL2(address, true, tbe.ct_id);        
        assert(L1Dcache.cleanup_done(address, tbe.ct_id) == true);
        sequencer.readCallback(address,L1Dcache.getConstDataBlock(), true, tbe.lineInL2);
        sequencer.cleanupAckCallback(address,tbe.ct_id);
        tbe.cleanup_inflight_done := true; //Since cleanupAck issued, cleanup table no longer available. 
        TBEs.deallocate(address);
        unset_tbe();              
      }
         
    }
  }

  
  action(dcil_done_cleanup_inflight_l2, "dcil",
         desc="Mark that Inflight Cleanup of L2 Done")
    {  
      L1Dcache.cleanup_set_doneEvictL2(address, true, tbe.ct_id);  
    }

  
  action(adc_assert_cleanup_done, "adc",
         desc="Ensure that Inflight Cleanup Done")
    {
      assert(L1Dcache.cleanup_done(address, tbe.ct_id) == true);
    }
 
  action(icrc_inflightcleanup_readCallback, "icrc", desc="issue readCallback when cleanup inflight complete")
    {
      if(L1Dcache.cleanup_done(address,tbe.ct_id)){
        sequencer.readCallback(address,L1Dcache.getConstDataBlock(), true, tbe.lineInL2);
      }
    }

  
  action(id_inflightcleanup_done, "id", desc="Notify Sequencer that cleanup is done")
    {
      if(L1Dcache.cleanup_done(address,tbe.ct_id)){
        sequencer.cleanupAckCallback(address,tbe.ct_id);
        tbe.cleanup_inflight_done := true; //Since cleanupAck issued, cleanup table no longer available. 
      }
    }

  //If cleanupAck issued, then cleanup_inflight_done = true. Then TBE can be deallocated.
  action(sdi_deallocateTBE_IfDoneCleanup, "sdi", desc="Deallocate TBE") {
    if(tbe.cleanup_inflight_done){ 
      TBEs.deallocate(address);
      unset_tbe();
    }
  }

  action(sci_skip_cleanup_inflight, "sci",
         desc="Skip Cleanup Inflight and Directly Issue CleanupAck?")
    {
      peek(mandatoryQueue_in, RubyRequest) {              

        DPRINTF(RubySlicc,"Skipping any cleanup for Addr:%#x CleanupInflight:Directly sending CleanupAck for MsgType:%s\n",address,in_msg.Type);

        L1Dcache.cleanup_set_toEvictL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneEvictL1(address, true, in_msg.ct_id);
        L1Dcache.cleanup_set_toInstallL1(address, false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneInstallL1(address, true, in_msg.ct_id);        
        L1Dcache.cleanup_set_toEvictL2(address,false, in_msg.ct_id);
        L1Dcache.cleanup_set_doneEvictL2(address,true, in_msg.ct_id);
        sequencer.readCallback(address,L1Dcache.getConstDataBlock(), true,false);
        sequencer.cleanupAckCallback(address,in_msg.ct_id);

      }

    }

  action(tdie_tbe_deallocate_ifexists, "tdie", desc="Deallocate TBE if exists") {
    
    peek(mandatoryQueue_in, RubyRequest) {              
      if(TBEs.isPresent(address)){
        
        //If TBE entry doesnt correspond to a cleanup or Inflight_Cleanup (self's) ct_id.
        if( (TBEs[address].ct_id == -1) || (TBEs[address].ct_id == in_msg.ct_id) ){
          TBEs.deallocate(address);
          unset_tbe();
        }
      }
    }
  }

  action(drp_drop_printf, "drp",
         desc="Drop Packet But Print What You Are Dropping!")
    {
      peek(responseL1Network_in, ResponseMsg) {
        DPRINTF(RubySlicc, "**Dropping Msg %s with address:%#x in State:%s \n",in_msg.Type, in_msg.addr,getState(TBEs[in_msg.addr],getCacheEntry(in_msg.addr), in_msg.addr));
      }
    }


  //*****************************************************
  // TRANSITIONS
  //*****************************************************

  // Transitions for Load/Store/Replacement/WriteBack from transient states
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK}, {Load, Expose, SpecLoad, Ifetch, Store, L1_Replacement}) {
    z_stallAndWaitMandatoryQueue;
  }

  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM}, L1_Replacement_OnInstall) {
    zzr_recycleResponseQueue;
  }


  transition({NP, I, S, E, M, IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM}, L1_Replacement_VictimBusy) {
    zzr_recycleResponseQueue;
  }


  transition({PF_IS, PF_IS_I}, {Store, L1_Replacement}) {
    z_stallAndWaitMandatoryQueue;
  }

  transition({PF_IM, PF_SM}, {Load, Expose, SpecLoad, Ifetch, L1_Replacement}) {
    z_stallAndWaitMandatoryQueue;
  }

  // Transitions from Idle
  transition({NP,I},{L1_Replacement, L1_Replacement_OnInstall}) {
    ff_deallocateL1CacheBlock;
  }

  transition({S,E,M,IS,IM,IX,SM,IS_I,M_I,SINK_WB_ACK,PF_IS,PF_IM},
             {PF_Load, PF_Store, PF_Ifetch}) {
    pq_popPrefetchQueue;
  }

  transition(I, Load, IS) {
    oo_allocateL1DCacheBlock; 
    i_allocateTBE;
    a_issueGETS;
    uu_profileDataMiss;
    po_observeMiss;
    k_popMandatoryQueue;
  }

  transition(NP, Load, NP) {
    //TODO: No Allocate L1DCacheBlock. Do this Later, when GetS returns. 
    iw_allocateTBEWithoutCacheEntry;
    a_issueGETS;
    uu_profileDataMiss;
    //Does not support Prefetching.
    k_popMandatoryQueue;
  }

  transition(NP,Delay_Load_RaceCondition){
    zzm_recycleMandatoryQueue;
  }

  transition({NP,I}, Expose, IS) {
    oo_allocateL1DCacheBlock;
    i_allocateTBE;
    ex_issueEXPOSE;
    uu_profileDataMiss;
    //po_observeMiss;
    k_popMandatoryQueue;
  }

  transition({NP,I}, SpecLoad, IX) {
    iw_allocateTBEWithoutCacheEntry;
    as_issueGETSPEC;
    k_popMandatoryQueue;
  }

  transition({NP,I}, PF_Load, PF_IS) {
    oo_allocateL1DCacheBlock;
    i_allocateTBE;
    pa_issuePfGETS;
    pq_popPrefetchQueue;
  }

  transition(PF_IS, {Load, Expose}, IS) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition(PF_IS_I, {Load, Expose}, IS_I) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition({PF_IS, PF_IS_I}, SpecLoad) {
    k_popMandatoryQueue;
  }

  transition({NP,I}, Ifetch, IS) {
    pp_allocateL1ICacheBlock;
    i_allocateTBE;
    ai_issueGETINSTR;
    uu_profileInstMiss;
    po_observeMiss;
    k_popMandatoryQueue;
  }

  transition({NP,I}, PF_Ifetch, PF_IS) {
    pp_allocateL1ICacheBlock;
    i_allocateTBE;
    pai_issuePfGETINSTR;
    pq_popPrefetchQueue;
  }

  // We proactively assume that the prefetch is in to
  // the instruction cache
  transition(PF_IS, Ifetch, IS) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition({NP,I}, Store, IM) {
    oo_allocateL1DCacheBlock;
    i_allocateTBE;
    b_issueGETX;
    uu_profileDataMiss;
    po_observeMiss;
    k_popMandatoryQueue;
  }

  transition({NP,I}, PF_Store, PF_IM) {
    oo_allocateL1DCacheBlock;
    i_allocateTBE;
    pb_issuePfGETX;
    pq_popPrefetchQueue;
  }

  transition(PF_IM, Store, IM) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition(PF_SM, Store, SM) {
    uu_profileDataMiss;
    ppm_observePfMiss;
    k_popMandatoryQueue;
  }

  transition({NP, I, IX}, Inv) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  // Transitions from Shared
  transition({S,E,M}, {Load, Expose}) {
    h_load_hit;
    uu_profileDataHit;
    k_popMandatoryQueue;
  }

  transition({S,E,M}, SpecLoad) {
    h_spec_load_hit;
    k_popMandatoryQueue;
  }

  transition({S,E,M}, Ifetch) {
    h_ifetch_hit;
    uu_profileInstHit;
    k_popMandatoryQueue;
  }

  transition(S, Store, SM) {
    i_allocateTBE;
    c_issueUPGRADE;
    uu_profileDataMiss;
    k_popMandatoryQueue;
  }

  transition(S,{L1_Replacement, L1_Replacement_OnInstall}, I) {
    forward_eviction_to_cpu;
    ff_deallocateL1CacheBlock;
  }

  transition(S, Inv, I) {
    forward_external_eviction_to_cpu;
    fi_sendInvAck;
    l_popRequestQueue;
  }

  // Transitions from Exclusive

  transition({E,M}, Store, M) {
    hh_store_hit;
    uu_profileDataHit;
    k_popMandatoryQueue;
  }

  transition(E,{L1_Replacement, L1_Replacement_OnInstall}, M_I) {
    // silent E replacement??
    forward_eviction_to_cpu;
    i_allocateTBE;
    g_issuePUTX;   // send data, but hold in case forwarded request
    ff_deallocateL1CacheBlock;
  }
  
  transition(E, Inv, I) {
    // don't send data
    forward_external_eviction_to_cpu;
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(E, Fwd_GETX, I) {
    forward_external_eviction_to_cpu;
    d_sendDataToRequestor;
    l_popRequestQueue;
  }

  transition(E, {Fwd_GETS, Fwd_GET_INSTR}, S) {
    d_sendDataToRequestor;
    d2_sendDataToL2;
    l_popRequestQueue;
  }

  transition({E, M}, Fwd_GETSPEC) {
    ds_sendDataToSpecRequestor;
    l_popRequestQueue;
  }

  transition({E, M}, Fwd_EXPOSE, S) {
    dex_sendDataToExposeRequestor;
    d2ex_sendExposeDataToL2;
    l_popRequestQueue;
  }

  // Transitions from Modified

  transition(M, {L1_Replacement, L1_Replacement_OnInstall}, M_I) {
    forward_eviction_to_cpu;
    i_allocateTBE;
    g_issuePUTX;   // send data, but hold in case forwarded request
    ff_deallocateL1CacheBlock;
  }

  transition(M_I, WB_Ack, I) {
    dil_done_if_l1invalidate; //Will mark L1Invalidate DOne if in progress
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(M, Inv, I) {
    forward_external_eviction_to_cpu;
    f_sendDataToL2;
    l_popRequestQueue;
  }
  
  transition(M_I, Inv, SINK_WB_ACK) {
    ft_sendDataToL2_fromTBE;
    l_popRequestQueue;    
  }

  transition(M, Fwd_GETX, I) {
    forward_external_eviction_to_cpu;
    d_sendDataToRequestor;
    l_popRequestQueue;
  }

  transition(M, {Fwd_GETS, Fwd_GET_INSTR}, S) {
    d_sendDataToRequestor;
    d2_sendDataToL2;
    l_popRequestQueue;
  }

  transition(M_I, Fwd_GETX, SINK_WB_ACK) {
    dt_sendDataToRequestor_fromTBE;
    l_popRequestQueue;
  }

  transition(M_I, {Fwd_GETS, Fwd_GET_INSTR}, SINK_WB_ACK) {
    dt_sendDataToRequestor_fromTBE;
    d2t_sendDataToL2_fromTBE;
    l_popRequestQueue;
  }

  transition(M_I, Fwd_EXPOSE, SINK_WB_ACK) {
    dtex_sendDataToExposeRequestor_fromTBE;
    d2tex_sendExposeDataToL2_fromTBE;
    l_popRequestQueue;
  }

  transition(M_I, Fwd_GETSPEC) {
    dts_sendDataToSpecRequestor_fromTBE;
    l_popRequestQueue;
  }

  // Transitions from ISX
  /*
    transition(ISX, Inv, IS_I) {
    fi_sendInvAck;
    l_popRequestQueue;
    //BUG: Subsequent to this, IS_I => Data_Exclusive -> E may be buggy in this context as the replacement to clear space has not yet occurred. But I think Inv is unlikely to appear in this context?
    }
  
    transition(ISX, Data_all_Acks, S) {
    u_writeDataToL1Cache; //**TODO** - write only after you have completed eviction?
    hx_load_hit; //If not rollback, calls sequencer with data (readCallback)
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
    }

    transition(ISX, DataS_fromL1, S) {
    u_writeDataToL1Cache; //**TODO** - write only after you have completed eviction?
    j_sendUnblock;
    hx_load_hit; //If not rollback, calls sequencer with data (readCallback)
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
    }

    transition(ISX, Data_Exclusive, E) {
    u_writeDataToL1Cache; //**TODO** - write only after you have completed eviction?
    hx_load_hit; //If not rollback, calls sequencer with data (readCallback)
    jj_sendExclusiveUnblock;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }
    */
  // Transitions from IS

  transition({IS, IS_I}, Inv, IS_I) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition({PF_IS, PF_IS_I}, Inv, PF_IS_I) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(IS, Data_all_Acks, S) {
    u_writeDataToL1Cache;
    hx_load_hit;
    dir_done_if_rollback;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS, Data_all_Acks, S) {
    u_writeDataToL1Cache;
    dir_done_if_rollback; 
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS_I, Data_all_Acks, I) {
    u_writeDataToL1Cache;
    hx_load_hit;
    dir_done_if_rollback; 
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS_I, Data_all_Acks, I) {
    dir_done_if_rollback; 
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, DataS_fromL1, S) {
    u_writeDataToL1Cache;
    j_sendUnblock;
    hx_load_hit;
    dir_done_if_rollback; 
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS, DataS_fromL1, S) {
    u_writeDataToL1Cache;
    j_sendUnblock;
    dir_done_if_rollback; 
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS_I, DataS_fromL1, I) {
    u_writeDataToL1Cache;
    j_sendUnblock;
    hx_load_hit;
    dir_done_if_rollback; 
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS_I, DataS_fromL1, I) {
    j_sendUnblock;
    dir_done_if_rollback; 
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // directory is blocked when sending exclusive data
  transition(IS_I, Data_Exclusive, E) {
    u_writeDataToL1Cache;
    hx_load_hit;
    dir_done_if_rollback; 
    jj_sendExclusiveUnblock;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // directory is blocked when sending exclusive data
  transition(PF_IS_I, Data_Exclusive, E) {
    u_writeDataToL1Cache;
    jj_sendExclusiveUnblock;
    dir_done_if_rollback; 
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(IS, Data_Exclusive, E) {
    u_writeDataToL1Cache;
    hx_load_hit;
    dir_done_if_rollback;
    jj_sendExclusiveUnblock;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IS, Data_Exclusive, E) {
    u_writeDataToL1Cache;
    jj_sendExclusiveUnblock;
    dir_done_if_rollback; 
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // [InvisiSpec] Data and Data_Exclusive are not possible at IX
  transition(IX, {Data_all_Acks, DataS_fromL1}, I) {
    hsx_spec_load_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }


  // Transitions from IM
  transition(IM, Inv, IM) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition({PF_IM, PF_SM}, Inv, PF_IM) {
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(IM, Data, SM) {
    u_writeDataToL1Cache;
    q_updateAckCount;
    o_popIncomingResponseQueue;
  }

  transition(PF_IM, Data, PF_SM) {
    u_writeDataToL1Cache;
    q_updateAckCount;
    o_popIncomingResponseQueue;
  }

  transition(IM, Data_all_Acks, M) {
    u_writeDataToL1Cache;
    hhx_store_hit;
    jj_sendExclusiveUnblock;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_IM, Data_all_Acks, M) {
    u_writeDataToL1Cache;
    jj_sendExclusiveUnblock;
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  // transitions from SM
  transition(SM, Inv, IM) {
    forward_external_eviction_to_cpu;
    fi_sendInvAck;
    dg_invalidate_sc;
    l_popRequestQueue;
  }

  transition({SM, IM, PF_SM, PF_IM}, Ack) {
    q_updateAckCount;
    o_popIncomingResponseQueue;
  }

  transition(SM, Ack_all, M) {
    jj_sendExclusiveUnblock;
    hhx_store_hit;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(PF_SM, Ack_all, M) {
    jj_sendExclusiveUnblock;
    s_deallocateTBE;
    mp_markPrefetched;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }

  transition(SINK_WB_ACK, Inv){
    fi_sendInvAck;
    l_popRequestQueue;
  }

  transition(SINK_WB_ACK, WB_Ack, I){
    dil_done_if_l1invalidate; //Will mark L1Invalidate DOne if in progress
    s_deallocateTBE;
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }


  //--------------------------------------------------------------------------------  
  //Transitions for AllocateLine_OnInstall

  transition(NP, AllocateLine_OnInstall, IS) {
    //Allocate Line
    oo_allocateL1DCacheBlock; 
    //Set the CacheEntry in tbe entry. 
    utoi_updateTBEOnInstall;
  }

  
  //***********************************
  // CLEANUP TRANSITIONS
  //***********************************

  //--------------------------------------------------------------------------------
  //Transitions on Waiting_CLEANUP event
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Waiting_CLEANUP) {
    zzm_recycleMandatoryQueue;
  }

  //--------------------------------------------------------------------------------
  //Transitions due to Inv_CLEANUP event
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM}, Inv_CLEANUP) {

    clw_cleanup_L1inv_waiting;
    zzm_recycleMandatoryQueue;
  }

  transition({NP, I}, Inv_CLEANUP) {
    //cls_cleanup_L1inv_success; //Invalidation in L1 successful
    clirs_cleanup_L1InvRoll_skip; //Skip invalidate L1 and rollback.
    //ff_deallocateL1CacheBlock;
    zzm_recycleMandatoryQueue;
  }

  transition({S, X}, Inv_CLEANUP, I) {
    cls_cleanup_L1inv_success; //Invalidation in L1 successful    
    ff_deallocateL1CacheBlock;
    zzm_recycleMandatoryQueue;
    
  }

  transition(E, Inv_CLEANUP, M_I) {
    clinvf_cleanup_L1inv_fired;    
    i_allocateTBE;
    mlinvt_mark_L1inv_TBE; //mark that current line is L1Invalidated in TBE.
    g_issuePUTX;   // putx to convey that we no-longer hold it.
    ff_deallocateL1CacheBlock;
    zzm_recycleMandatoryQueue;
  }

  transition(M, Inv_CLEANUP) {
    clirs2_cleanup_L12InvRoll_skip; //Skip L1 / L2 invalidate and rollback
    //suc_screwed_up_cleanup;
    zzm_recycleMandatoryQueue;
  }

  //--------------------------------------------------------------------------------
  //Transitions due to Done_Invalidate_CLEANUP event
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Done_Invalidate_CLEANUP, I) {    
    cls_cleanup_L1inv_success; //Invalidation in L1 successful
    zzm_recycleMandatoryQueue;
  }
    
  //Transitions on Waiting_Rollback_CLEANUP event
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Waiting_Rollback_CLEANUP) {
    clww_cleanup_L1rollback_waiting1;
    zzm_recycleMandatoryQueue;
  }

  //--------------------------------------------------------------------------------
  //Transitions for Load_ROLLBACK event
  transition({NP,I}, Load_ROLLBACK, IS) {
    oo_allocateL1DCacheBlock;
    i_allocateTBE;
    a_issueGETS;
    clf_cleanup_L1rollback_fired;
    mrt_mark_rollback_TBE;
    zzm_recycleMandatoryQueue;
  }
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM}, Load_ROLLBACK) {
    clrw_cleanup_L1rollback_waiting2;
    zzm_recycleMandatoryQueue;
  }
  
  transition({S,E,M,X}, Load_ROLLBACK) {    
    clf_cleanup_L1rollback_fired;
    cld_cleanup_L1rollback_done;
    zzm_recycleMandatoryQueue;
  }

 
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Skip_ROLLBACK) {
    clf_cleanup_L1rollback_fired;
    cld_cleanup_L1rollback_done;
    zzm_recycleMandatoryQueue;
  }

  //--------------------------------------------------------------------------------  
  //Transitions for InvL2_CLEANUP event
  //TODO: All to itself transition - send invalidation request to L2 Bank, Toggle Flag, How to Toggle 2nd Flag when response received?
  transition({NP,I} ,InvL2_CLEANUP) {
    ilf_invl2_fired; //Mark toEvict_L2 = true; L2-invalidation started.
    iwac_allocateTBEWithoutCacheEntry_CleanupExec; //allocate TBE if not already existing. If exists, then ct_id should match.
    mtrl_markTBE_receive_L2invack;   
    ipi_issuePUTinv;  
    zzm_recycleMandatoryQueue;    
  }

  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, M},InvL2_CLEANUP) {        
    cclw_cleanup_L2invalidate_waiting; //Wait while line is not in NP
    zzm_recycleMandatoryQueue;
  }

  transition({I,NP}, L2InvAckRecvd) {
    ild_invl2_done; //Mark doneEvict_L2 = true;
    s_deallocateTBE;
    o_popIncomingResponseQueue;
  }

  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X,  M}, L2InvAckRecvd) {        
    cclw_cleanup_L2invalidate_waiting; //Wait while line is not in NP
    zzr_recycleResponseQueue;    
  }




  //--------------------------------------------------------------------------------  
  //Transitions for Done_CLEANUP
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Done_CLEANUP) {
    cd_cleanup_done;
    k_popMandatoryQueue;
  }

  //Transitions for Waiting_Final_CLEANUP
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Waiting_Final_CLEANUP) {
    cfw_cleanup_final_waiting;
    zzm_recycleMandatoryQueue;
  }

  /*
  //Transition when we screw up:
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Screwed_UP) {
  suc_screwed_up_cleanup;
  }


  //Transition for Rollabck Load to L1
  */

  
  //--------------------------------------------------------------------------------  
  //Transitions for Waiting_cleanup_
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Waiting_CLEANUP_inprogress) {
    ciw_cleanup_inprogress_waiting;
    zzm_recycleMandatoryQueue;
  }

  //--------------------------------------------------------------------------------  
  //Transitions for Waiting_Replacement
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Waiting_REPL_inprogress) {
    riw_repl_inprogress_waiting;
    zzm_recycleMandatoryQueue;
  }

  //--------------------------------------------------------------------------------  
  //Transitions for Inflight_CLEANUP
  transition(NP, Inflight_CLEANUP_reqs) {
    tcis_tbe_cleanup_inflight_set;//set TBE bit to say inflight cleanup occurred.       
    cis_cleanup_inflight_stateupdate;//update state in CleanupTable (toEvictL1, toInstallL1, doneEvictL1, doneEvictL1)
    icillc_issue_cleanup_inflight_l2;//issue cleanupinflight request to L2, if required. Update toEvictL2, doneEvictL2.

    //If cleanupdone, complete cleanup:
    icrc_inflightcleanup_readCallback;//issue readCallback
    id_inflightcleanup_done;//issue cleanupAckCallback.
    sdi_deallocateTBE_IfDoneCleanup; //Deallocate TBE.
    
    k_popMandatoryQueue; //Done inflight cleanup
        
    //Separately:
    //**TODO**: Handle the MESI-L2 file changes: triggering action to invalidate inflight request / dropping hit.
  }

  transition(NP, Inflight_CLEANUP_resp) {
    dcil_done_cleanup_inflight_l2;//update doneL2evict.
    adc_assert_cleanup_done; //make sure cleanup is done.

    icrc_inflightcleanup_readCallback;//issue readCallback
    id_inflightcleanup_done;//issue cleanupAckCallback
    sdi_deallocateTBE_IfDoneCleanup; //Once everything is complete, deallocate TBE.    
    o_popIncomingResponseQueue;
  }

  // ############# CORNER CASE #############
  // When Cleanup_Req and Load are both in Mandatory Queue: 
  //Transition 1:
/*
  transition(M_I, Inflight_CLEANUP_reqs) {
    zzm_recycleMandatoryQueue;
  }
*/

  transition({M_I,I}, Inflight_CLEANUP_reqs, NP) {
    sci_skip_cleanup_inflight;    
    tdie_tbe_deallocate_ifexists ; //only deallocate if TBE its own, or a non-ongoing cleanup.
    k_popMandatoryQueue; //Done inflight cleanup
  }


  //For Scenarios where Inflight Load has not even created TBE.
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Skip_Inflight_Cleanup) {
    sci_skip_cleanup_inflight;    
    k_popMandatoryQueue; 
  }


  //Transition 2:
  transition({IS, IM, IX, IS_I, M_I, SM, SINK_WB_ACK, PF_IS, PF_IS_I, PF_IM, PF_SM, S, E, X, NP, I, M}, Drop_Load_Cleanup_Inflight) {    
    k_popMandatoryQueue;
  }

  //########### DONE CORNER CASE ############

  transition(NP, DropIncomingPacketnUnblockL2) {
    drp_drop_printf;
    jj_sendExclusiveUnblock;
    o_popIncomingResponseQueue;
  }

  transition(NP, DropIncomingPacketnUnblockRevertL2) {
    drp_drop_printf;
    jjr_sendExclusiveUnblockRevert;
    mtlr_markTBEL2HitReceived;
    cia_cleanup_ack_ifwaitingonUnblock;
    o_popIncomingResponseQueue;
  }

  transition({NP, M_I}, DropIncomingPacket) {
    drp_drop_printf;
    mtlr_markTBEL2HitReceived;
    //cia_cleanup_ack_ifwaitingonUnblock;
    o_popIncomingResponseQueue;
  }

  transition({IS,IM,NP}, WB_Ack) {
    drp_drop_printf;
    dil_done_if_l1invalidate; //Will mark L1Invalidate DOne if in progress 
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;
  }



  transition(IM, Data_Exclusive) {
    jjr_sendExclusiveUnblockRevert; 
    o_popIncomingResponseQueue;
    kd_wakeUpDependents;        
  }



}

